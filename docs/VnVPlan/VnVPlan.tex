\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Nov 5, 2024 & Rev 0 & Initial Draft\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables


\newpage

\section{Symbols, Abbreviations, and Acronyms}

All symbols, abbreviations, and acronyms can be found within section E.1 (Glossary) of the \href{https://github.com/takhtart/PCD/blob/main/docs/SRS/SRS.pdf}{SRS Report}.

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation  plan for our software project. 
The purpose of this plan is to increase confidence in our software by ensuring it meets its requirements and performs as expected. 
This plan will list our objectives and processes related to verification and validation of our system and our roadmap for doing so.

\section{General Information}

\subsection{Summary}

The software being tested is our Partially Covered Detection (PCD) system. 
The system leverages depth and RGB layers to form a combined coloured point cloud, which is then analyzed to accurately detect individuals even when they are not fully visible. 
Detection can occur in a live setting through a Kinect, or using offline file captures.

\subsection{Objectives}

The primary objective of this VnV plan is to ensure the system's correctness and performance, verifying both its functional and non-functional requirements. 
This involves testing the system's ability to accurately identify partially obscured individuals and demonstrate that it operates efficiently within its environment. 
Additionally, the plan aims to ensure that the system implementation matches the project specifications. Testing within dark environments is considered out of scope, as RGB data cannot be captured in such settings. 
Furthermore, the project assumes that any external libraries used have already been verified by their respective implementation teams.

\subsection{Challenge Level and Extras}

The challenge level for this project is advanced, as agreed upon with our assigned TA. A User Manual and Design Thinking additions are our included extras.

\newpage

\subsection{Relevant Documentation}

The following documents are relevant to the Verification and Validation efforts for this project. 
Each document listed below provides information supporting the VnV process, ensuring that the system meets its requirements and operates as intended.

\begin{enumerate}

  \item \href{https://github.com/takhtart/PCD/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement and Goals}: This document outlines the primary objectives and challenges of the project. 
  It provides a clear understanding of the problem being addressed and the goals to be achieved, and is vital for defining the scope and focus of the VnV proceedings.

  \item \href{https://github.com/takhtart/PCD/blob/main/docs/DevelopmentPlan/DevelopmentPlan.pdf}{Development Plan}: This plan includes risks related to the project prior to conducting a formal hazard analysis. 
  It is important to verify the risks outlined in the document relating to the software system have been mitigated.
  
  \item \href{https://github.com/takhtart/PCD/blob/main/docs/SRS/SRS.pdf}{SRS Report}: The Software Requirements Specification (SRS) report defines the functional and non-functional requirements of the system. 
  It includes a brief baseline for VnV efforts, providing the criteria against which the system's correctness and performance will be evaluated.
  
  \item \href{https://github.com/takhtart/PCD/blob/main/docs/HazardAnalysis/HazardAnalysis.pdf}{Hazard Analysis}: This document identifies potential hazards and risks associated with the system. 
  It is essential for the VnV process as it helps in prioritizing the testing efforts to address the most critical risks and ensure the system's safety and reliability.
  
  \item \href{https://github.com/takhtart/PCD/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{Module Interface Specification}: The Module Interface Specification (MIS) describes the interfaces between different modules of the system. 
  It is relevant to the VnV efforts as it ensures that the interactions between modules are correctly implemented and function as intended.
  
  \item \href{https://github.com/takhtart/PCD/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide}: The Module Guide (MG) provides detailed descriptions of each module's design and implementation. 
  It is used in the VnV process to verify that each module meets its design specifications and integrates seamlessly with other modules.
  
  \item \href{https://github.com/takhtart/PCD/blob/main/docs/VnVReport/VnVReport.pdf}{VnV Report}: This report documents the results of the VnV activities, including the tests performed, issues identified, and their resolutions. 
  It provides a comprehensive evaluation of the system's compliance with its requirements and serves as a record of the VnV efforts.

\end{enumerate}

\section{Plan}

This section describes the overall plan for the verification and validation of our system. It includes the work breakdown 
of each member of the verification and validation team. This section also outlines the plans for the verification of 
our SRS, Design, and VnV. Furthermore, it details the plans for the implementation of these verification strategies as well as 
the implementation of the testing tools and the software validation plan.

\subsection{Verification and Validation Team}

  \begin{table}[H]
  \caption{Verification and Validation Team Members Table}
  \centering
  \begin{tabular}{|l|p{1.8in}|p{2.5in}|}
  \hline
  \textbf{Name}            & \textbf{Role(s)}                                       & \textbf{Responsibilities}                                                                                                                                             \\ \hline
  Harman Bassi         & Lead test developer, Test developer, Manual tester               & Lead the test development process. Create automated tests for backend code. Main verification and reviewer of system/unit tests. \\ \hline
  Matthew Bradbury             & Test developer, Manual tester, Code Verifier       & Create automated tests for backend code. Manually test human detection algorithm functionality. Ensure source code follows project coding standard. Verification reviewer for the Hazard Analysis and SRS.                                                           \\ \hline
  Kyen So         & Test developer, Manual tester, Code Verifier                & Create automated tests for backend code. Manually test human outline manager functionality. Ensure source code follows project coding standards. Main verification reviewer for the Verification and Validation document.                                                                                               \\ \hline
  Tarnveer Takhtar            & Test developer, Manual tester        & Create automated tests for backend code. Manually test Kinect manager and ensure proper functionality. Verification Reviewer of Hazard Analysis and SRS                                       \\ \hline
  Dr. Gary Bone & Supervisor, SRS validator, Final reviewer &  Make sure SRS meets requirements of the project, Validate code functionality. Because Dr. Bone is the supervisor of this project, he can verify that the project is functioning as expected.\\ \hline
  \end{tabular}
  \end{table}

\subsection{SRS Verification Plan}

The current plan to verify our SRS involves incorporating both self-review and peer-review feedback, used in tandem with notes from our TA 
and a final read-over with our supervisor. Our team will first do a quick read-through of each other's sections and provide feedback for changes
in the form of comments on the issue. We will then incorporate the feedback we receive from our peers in another group, delivered to us via 
separate Github issues. These issues will be assigned to a single member of the team, who will have the responsibility of finishing and closing it.
Additionally, we will create issues related to the feedback we received from our TA and work on adding the corresponding changes to our SRS. 
Finally, after incorporating all the feedback received, we will host a meeting with Dr. Bone. In this meeting, the team will walk Dr. Bone through
our SRS and get his opinion on any final changes that need to be made to our requirements. Issues will be created to address the requested changes.


\subsection{Design Verification Plan}

Like the SRS, the plan for design verification includes a team review, a peer review, and a TA review. Like the SRS, the 
team review will involve team members reviewing another team member's section and commenting on changes that should be made. 
The peer review will similarly consist of another team adding Github issues to our repository and getting assigned to a team member. 
At that time, the specified team member will complete and close the issue. Additionally, we will incorporate issues raised by our TA. 

\subsection{Verification and Validation Plan Verification Plan}

Similarly to the previous plans, this one will consist of a team review, a peer review, and a TA review, as well as a review from our
supervisor. Just as the previous plans, the team, peer, and TA review will be conducted in the same way as previously mentioned. Additionally,
for this verification plan, we will also be including our supervisor, Dr. Bone. The team will schedule a meeting with him and go through
specific parts of the document. These parts will be our plan for what automated tests we are implementing, and Dr. Bone will have the final word
on any improvements or changes we should make before proceeding.

\subsection{Implementation Verification Plan}

For our implementation verification plan, we will perform all of the tests from the corresponding set of system tests in 4.1.1, 4.1.2 and 4.1.4 with every version of the code that makes changes to either the human detection component or the 3D space estimation component. This allows us to verify that our changes continues to meet our functional requirements. 
For major version updates, we will perform all the tests in 4.1.3 and 4.1.5, in addition to all of the previously mentioned tests so that our entire test suite of 4.1 is covered, ensuring that all of our functional requirements are met. Additionally, we will perform every test 4.2 for every major version update to ensure that out non-functional requirements are met. Performing our entire test suite for every major version update ensures that our software is working as intended. Despite most of our tests being manual, many of them aren't particularly time consuming.
Finally, for every major version update, a code walkthrough will be performed by the developers and Dr. Gary Bone so that we can find out if there are any discrepancies between what had been implemented and the stakeholder's needs. 

\subsection{Automated Testing and Verification Tools}

For the automated testing, cppunit will be used as it provides a simple and portable way to unit test the system. When it comes to doing coverage testing it would be best to use GCov because it is compatible with VScode and easier to set up compared to other applications. The main coverage focus for the project would be MC/DC coverage because it is a good coverage test for complicated decisions which the PCD system will have to make.
Linters are also a good tool to help ensure all the code meets a certain standard that is respected within the field. For this project, Clang-Tidy will be used because it is compatible with VS code and meets the standards within the industry for C++.


\subsection{Software Validation Plan}

For software validation, we will be providing Dr. Bone with bi-weekly written updates on the progress of the project. These updates
will serve as a brief validation that our software matches the aforementioned requirements outlined in the SRS. These smaller written checkups serve
as an iterative way to validate the software against the requirements by constantly getting input from Dr. Bone about new code.
Larger releases, such as code milestones or demos, will be accompanied by a meeting with Dr. Bone instead of a written update. 
These meetings will be lead by the main developers of the corresponding section and serve as the main form of software validation. 
The team will also consider peer feedback and TA/prof feedback from the demo to determine if the software accomplishes its goals.

\section{System Tests}

System tests are divided into tests for functional and non-functional requirements. Each functional and non-functional requirement we have will be sufficiently tested to ensure that these requirements are properly implemented and integrated into our system.

\subsection{Tests for Functional Requirements}

Each subsection below covers a functional requirement of the system. All functional requirements are accounted for and each have its own tests such that all functional requirements are met. 

\subsubsection{Human Detection Testing}
		
\paragraph{Live Tests}

\begin{enumerate}

\item{Live Full body, Uncovered Test (FT11)\\}

\textbf{Control:} Manual
					
\textbf{Initial State:} Kinect is properly set up whilst facing an environent without any objects that may obstuct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame.
					
\textbf{Input:} Realtime PCD from Kinect
					
\textbf{Output:} The software recognizes that there is a human in frame in realtime.

\textbf{Test Case Derivation:} The software is expected to first not detect any humans while no one is in frame. Once the human is fully in frame, the software is expected to recognize that someone is now in frame in order to sastify the requirement of human detection.
					
\textbf{How test will be performed:} The test begins with an environment free of any objects that may obstruct the view of a person and the software running in realtime. The tester will ensure that the software doesn't detect any human while no one is in frame. Then a human will be instructed to walk into frame. The output of the software will be inspected and analyzed once the human is fully in frame. 
					
\item{Live Upper Body, Uncovered Test (FT12)\\}

\textbf{Control:} Manual
					
\textbf{Initial State:} Kinect is properly set up whilst facing an environent without any objects that may obstuct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame close to the kinect such that only their upper body is visible.
					
\textbf{Input:} Realtime PCD from Kinect
			
\textbf{Output:} The software recognizes that there is a human in frame in realtime.

\textbf{Test Case Derivation:} The software is expected to first not detect any humans while no one is in frame. Once the top half of the human is fully in frame, the software is expected to recognize that someone is now in frame in order to sastify the requirement of human detection.
					
\textbf{How test will be performed:} The test begins with an environment free of any objects that may obstruct the view of a person and the software running in realtime. The tester will ensure that the software doesn't detect any human while no one is in frame. Then a human will be instructed to walk into frame close to the kinect such that only their upper body is visible. The output of the software will be inspected and analyzed once the human is fully in frame. 
	
\item{Live Human Partially Covered by Another Human Test (FT13)\\}

\textbf{Control:} Manual

\textbf{Initial State:} Kinect is properly set up whilst facing an environment without any objects that may obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. Two humans are ready to walk into frame.

\textbf{Input:} Realtime PCD from Kinect

\textbf{Output:} The software correctly recognizes and distinguishes 2 humans separately in realtime

\textbf{Test Case Derivation:} The software is expected to first not detect any humans while no one is in frame. Once both humans are fully in frame and one human is partially covering the other, the software is expected to recognize that there are two humans in frame and be able to distinguish each human in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins with an environment free of any objects that may obstruct the view of a person and the software running in realtime. The tester will ensure that the software doesn't detect any human while no one is in frame. Then, both humans will be instructed to walk into frame with one human partially covering the other. The output of the software will be inspected and analyzed once both humans are fully in frame and in the correct position.

\item{Live Human Partially Covered by Another Object Test (FT14)\\}

\textbf{Control:} Manual

\textbf{Initial State:} Kinect is properly set up whilst facing an environment with an object placed that is large enough to partially obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame.

\textbf{Input:} Realtime PCD from Kinect

\textbf{Output:} The software recognizes that there is a human in frame in realtime.

\textbf{Test Case Derivation:} The software is expected to first not detect any humans while no one is in frame. Once the human is fully within the frame and is partially covered by the object, the software is expected to recognize that there is a human behind the object in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins in an environment that contains an object big enough to partially obstruct a person from the Kinect’s view, such as a chair or table, and with the software running in realtime. The tester will ensure that the software doesn't detect any human while no one is in frame. Then, a human will be instructed to walk into frame and stand behind the object. The output of the software will be inspected and analyzed once the human is in the correct position.


\end{enumerate}

\paragraph{Offline Tests}

\begin{enumerate}
  
\item{Offline Full Body, Uncovered Test (FT15)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

\textbf{Input:} .pcd file containing the full-body of a human unobstructed by any objects .

\textbf{Output:} The software recognizes that there is a human in frame

\textbf{Test Case Derivation:} Given a .pcd file input, the software is expected to be able to analyze the data in the file. Since the .pcd file contains the full-body of a human, the software is expected to recognize that there is a human in the frame in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins by first obtaining a .pcd file that contains the full-body of a human unobstructed by any objects. Then, the .pcd file is uploaded to the software. The output of the software will be inspected and analyzed by the tester.

\item{Offline Upper Body, Uncovered Test (FT16)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

\textbf{Input:} .pcd file containing only the upper body of a human unobstructed by any objects .

\textbf{Output:} The software recognizes that there is a human in frame

\textbf{Test Case Derivation:} Given a .pcd file input, the software is expected to be able to analyze the data in the file. Since the .pcd file contains the upper body of a human, the software is expected to recognize that there is a human in the frame in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins by first obtaining a .pcd file that contains only the upper body of a human unobstructed by any objects. Then, the .pcd file is uploaded to the software. The output of the software will be inspected and analyzed by the tester.

\item{Offline Human Partially Covered by Another Human Test (FT17)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

\textbf{Input:} .pcd file containing 2 humans with one human partially covering the other.

\textbf{Output:} The software correctly recognizes and distinguishes 2 humans separately

\textbf{Test Case Derivation:} Given a .pcd file input, the software is expected to be able to analyze the data in the file. Since the .pcd file contains 2 humans with 1 partially covering the other, the software is expected to recognize and distinguish each human in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins by first obtaining a .pcd file that contains 2 humans with 1 partially covering the other. Then, the .pcd file is uploaded to the software. The output of the software will be inspected and analyzed by the tester.

\item{Offline Human Partially Covered by Another Object Test (FT18)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

\textbf{Input:} .pcd file containing a human who is partially covered by an object such as a chair or a table

\textbf{Output:} The software recognizes that there is a human in frame

\textbf{Test Case Derivation:} Given a .pcd file input, the software is expected to be able to analyze the data in the file. Since the .pcd file contains a human partially covered by an object, the software is expected to recognize that there is a human in frame in order to satisfy the requirement of human detection.

\textbf{How test will be performed:} The test begins by first obtaining a .pcd file that contains a human partially covered by an object such as a chair or table. Then, the .pcd file is uploaded to the software. The output of the software will be inspected and analyzed by the tester.

\end{enumerate} 


\subsubsection{3D Space Estimation}

\paragraph{Live Tests}
\begin{enumerate}
  \item{Full Body, Uncovered (FT21)\\}

  \textbf{Control:} Manual

  \textbf{Initial State:} Kinect is properly set up whilst facing an environment without any objects that may obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame.
  
  \textbf{Input:} Realtime PCD from Kinect
  
  \textbf{Output:} The software locates the human outline of the human’s head, torso, and limbs(arms and legs) within the 3D environment. 
  
  \textbf{Test Case Derivation:} The software would first expect to detect no one within the empty environment. Then when the human walks into frame the software is expected to determine the outline of the human and identify the three main components of the human, the head, torso, and limbs. These components would be outlined separately showing it within the 3D space.
  
  \textbf{How test will be performed:} The test will first have the empty  environment and then have a human fully appear in frame with nothing covering them. The tester would then ensure that the screen does not detect any humans in the frame. Then the human would walk in front of the sensor making sure that their whole body would be visible in front of the Kinect. The tester would then check the screen to see that the system outlines the human’s head, torso and limbs.

  \item{Upper Body, Uncovered (FT22)\\}
  
  \textbf{Control:} Manual
  
  \textbf{Initial State:} Kinect is properly set up whilst facing an environment without any objects that may obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame.
  
  \textbf{Input:} Realtime PCD from Kinect
  
  \textbf{Output:} The software locates the human outline of the human’s head, torso, and arms (if in frame) within the 3d environment. 
  
  \textbf{Test Case Derivation:} The software would first expect to detect no one within the empty environment. Then when the human walks into frame, but only with the upper body being visible. The software is then able to recognize that there is a human and outline only the head, torso and possibly the arms.These components would be outlined separately showing it within the 3D space.
  
  \textbf{How test will be performed:} The test will first have the empty  environment. The tester would then ensure that the screen does not detect any humans in the frame. Then the human would walk in front of the sensor making sure that only their upper body is shown to the Kinect. The tester would then check the screen to see if  the system outlines the human’s head, torso and possibly the arms if visible within frame.
  
  \item{Human Covered by Another Human (FT23)\\}
  
  \textbf{Control:} Manual

  \textbf{Initial State:} Kinect is properly set up whilst facing an environment without any objects that may obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. Person 1  is ready to walk into frame and person 2 is ready to walk behind person 1.
  
  \textbf{Input:} Realtime PCD from Kinect

  \textbf{Output:} The software shows the human at the front and outlines the head, torso,and limbs are visible. Also outlines the body parts of the human behind the first human. 

  \textbf{Test Case Derivation:} Because two humans are visible on the screen, the system is expected to outline person 1’s head, torso, and limbs. Then it should separately outline person 2’s visible parts (the head, torso, and visible limbs).

  \textbf{How test will be performed:} The two humans would be visible within the environment and the tester would make sure that person 1 is partially visible behind person 2 on screen. The system should then identify that there are two people visible on screen and outline the head, torso, and limbs of person 1. Then it would also identify the head, torso, and possibly the arms of person 2. 

  \item{Human Partially covered by another object (FT24)\\}

  \textbf{Control:} Manual

  \textbf{Initial State:} Kinect is properly set up whilst facing an environment with objects that obstruct the view of a person. Software is running in realtime and is not detecting anyone in frame. A person is ready to walk into frame and hide behind the object.

  \textbf{Input:} Realtime PCD from Kinect.

  \textbf{Output:} The software outlines the visible parts of the human. The human will only show their head and part of their torso.

  \textbf{Test Case Derivation:} The object within the screen does not create any separate outline. When the person has their head and torso visible it is expected that the system is able to detect the human and figure out that the head and torso are visible. These body parts are then outlined and displayed on screen.

  \textbf{How test will be performed:} The object should be visible on screen by itself. Then the human goes and hides behind the object only displaying parts of themselves. The tester then is able to check the screen and be able to see that the human has their head and torso outlined.
  
\end{enumerate}

\paragraph{Offline Tests}
\begin{enumerate}
  \item{Full Body, Uncovered (FT25)\\}
  
  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.
  
  \textbf{Input:}  A  file of a human that is in full view and is not obstructed by any object. 

  \textbf{Output:} The software locates the human outline of the human’s head, torso, and limbs(arms and legs) within the 3D environment. 
  
  \textbf{Test Case Derivation:} The file uploaded should have the human be fully visible in the environment and so when the file is uploaded it is expected that the system is able to detect that there is a human in the frame and outline all the  body parts.This is because the system is able to see the whole body and so it should be able to outline the parts.
  
  \textbf{How test will be performed:} The correct file is uploaded into the system and the result should be displayed on screen. The human should have their head, torso, and limbs outlined.The screen should display this for the tester, who will check the file and match with the screen.
  
  \item{Upper Body, Uncovered (FT26)\\}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

  \textbf{Input:} File of a human that is only showing the upper body and is not obstructed by any object.

  \textbf{Output:} The software locates the human within a well outputted 3D estimation.

  \textbf{Test Case Derivation:} The system reads the uploaded file and is expected to display the upper body of the human (head and torso) because that is all it is able to see in the given frame.

  \textbf{How test will be performed:} The file gets uploaded in the offline mode. The system then processes the file and outputs the outline of the human's upper body on screen. The head and torso will be outlined within the 3D environment.The screen should display this for the tester, who will check the file and match with the screen.
  
  \item{Human Covered by Another Human (FT27)\\}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.
  
  \textbf{Input:} File of a human that is being covered by someone else in frame.
  
  \textbf{Output:} The system locates the human within a well outputted 3D estimation.
  
  \textbf{Test Case Derivation:} The system reads the uploaded file and then the system is expected to display the two humans separately and outline the human in the front fully and the head and torso of the human in the back. This is because in the file uploaded the system is able to clearly see the human in the front while the person in the back is partially hidden.
  
  \textbf{How test will be performed:}  The file gets uploaded in the offline mode. The system then processes the file and outputs the outline of the human's full body on screen. It also recognizes the second person and is able to outline their head and torso. The screen should display this for the tester, who will check the file and match with the screen.

  \item{Human Partially covered by another object (FT28)\\}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in offline mode which allows the user to upload a .pcd file.

  \textbf{Input:} File of a human partially hidden behind an object.

  \textbf{Output:} The system locates the humans visible body parts within a well outputted 3D estimation.

  \textbf{Test Case Derivation:} Because the human is only partially visible (the  head and torso within the file), the system is only able to outline the head and the torso of the human on screen.

  \textbf{How test will be performed:} The file gets uploaded in the offline mode. The system will process the file and outline the head and torso of the human hidden behind the object. The screen should display this for the tester, who will check the file and match with the screen.

  
  
\end{enumerate}

\subsubsection{Offline Processing}

\paragraph{File Format Tests}
\begin{enumerate}
\item{.pcd File Test (FT31)\\}
  
\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a file.

\textbf{Input:} Any .pcd file

\textbf{Output:} The software is able to read and analyze the data from the uploaded .pcd file and notifies the user that the file has been successfully uploaded

\textbf{Test Case Derivation:} The software should be able to analyze and read the data from any given .pcd file. It should recognize that the correct file format has been uploaded.

\textbf{How test will be performed:} The system will be in offline mode and the tester will upload a .pcd file. The feedback of the software will be inspected by the tester.

\item{Incorrect File Format Test (FT32)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode which allows the user to upload a file.

\textbf{Input:} Any file that isn’t a .pcd file such as a .jpg file

\textbf{Output:} The software is unable to read and analyze the data from the uploaded file and notifies the user that the file upload was unsuccessful

\textbf{Test Case Derivation:} The software should be not able to analyze and read the data from a file format that isn’t .pcd. It should recognize that the incorrect file format has been uploaded.

\textbf{How test will be performed:} The system will be in offline mode and the tester will upload a file that isn’t a .pcd file. The feedback of the software will be inspected by the tester.

\end{enumerate} 

\subsubsection{Body Pose Variation Handling}

\begin{enumerate}
  \item{Person not facing the sensor (FT41)}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in real time with no object obstructing the view of the sensor. The person is facing away from the sensor.
  
  \textbf{Input:} Realtime PCD from Kinect

  \textbf{Output:} The system should be detected and their head, torso ,and limbs will be outlined on screen.

  \textbf{Test Case Derivation:} The outcome should be the same as the person looking at the sensor. It should not change the outcome and so the system is expected to behave the same as before and outline the human.

  \textbf{How test will be performed:} The person would stand in the open of the environment and face away from the sensor. The system will then outline the person’s body parts and display the results on the screen. The tester can then verify if it is the correct output.

  \item{Person sitting on chair (FT42)}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in real time with the chair obstructing the part of the person in the frame. The person is sitting facing away from the sensor.

  \textbf{Input:} Realtime PCD from Kinect 

  \textbf{Output:} The system should be detected and their head and torso will be outlined on screen.

  \textbf{Test Case Derivation:} Because the person is sitting it should not affect the outcome of the human. The chair is blocking part of the human and only the head and torso would be visible so that is the only thing that the sensor should be able to pick up.

  \textbf{How test will be performed:}  The person would sit in the frame facing away from the sensor. The system is then able to detect the part of the human visible and outline the head and torso. The tester is able to see this on the screen and verify the outcome.

  \item{Poking Head In and Out (FT43)}

  \textbf{Control:} Manual

  \textbf{Initial State:} The system is running in real time with an object fully obstructing the person.

  \textbf{Input:} Realtime PCD from Kinect.

  \textbf{Output:} The system should be detected and their head whenever it is poked out.

  \textbf{Test Case Derivation:} The system is updating in real time and so it should be able to display the head outline whenever the sensor picks it up.

  \textbf{How test will be performed:} The person would be hidden behind the objects and then would repeatedly poke their head out. The system will then outline the head whenever it is in frame. The tester will observe this on the screen and see if the head is outlined at the correct times.
\end{enumerate} 

\subsubsection{Integration with Kinect Sensor}
\paragraph{Live Tests}
\begin{enumerate}
\item{Live Connection Test (FT51)}
  
\textbf{Control:} Manual

\textbf{Initial State:} Kinect is turned on and connected to the computer that the software is running from. 

\textbf{Input:} Disconnection and reconnection to Kinect

\textbf{Output:} When the kinect is disconnected ,the system recognizes that the Kinect is disconnected and notifies the user. Once the kinect is connected again, the system recognizes that the kinect is connected and notifies the user

\textbf{Test Case Derivation:} The software should be able to integrate seamlessly with the Kinect meaning that if the connection to the Kinect is severed, the software should be aware and notify the user. Once the Kinect is reconnected, the system will immediately be aware that the Kinect is connected and notify the user making the connection seamless.

\textbf{How test will be performed:} The test begins with the Kinect already connected to the system. The tester would check to see that the Kinect is connected properly and then disconnect the connection. The tester will reconnect the Kinect to the computer that the software is running from and inspect the connection status of the Kinect in the system.

\item{Live Kinect Data Test (FT52)}

\textbf{Control:} Manual

\textbf{Initial State:} Kinect is turned on and connected to the computer that the software is running from. 

\textbf{Input:} Realtime PCD from Kinect

\textbf{Output:} Stable stream of realtime PCD from Kinect visible to the user in the software

\textbf{Test Case Derivation:} The software should be able to read PCD from the Kinect in realtime

\textbf{How test will be performed:} The test begins with the Kinect already connected to the system and facing any environment. The tester would make hand gestures in front of the kinect sensor and then inspect to see if the PCD that the software reads accurately represents what is happening inside the frame of the Kinect in realtime.

\end{enumerate} 


\subsection{Tests for Nonfunctional Requirements}

Each subsection below covers a nonfunctional requirement of the system. All nonfunctional requirements are accounted for and each have its own tests such that all nonfunctional requirements are met.

\subsubsection{Realtime Processing}

\begin{enumerate}

\item{Poking Head Out Test(NFT11)\\}

\textbf{Control:} Manual

\textbf{Initial State:} The system is running with an object fully obstructing the person.

\textbf{Input:} Realtime PCD from Kinect.

\textbf{Output:} The system should be detected and their head whenever it is poked out within a certain amount of time.

\textbf{Test Case Derivation:} The system should be updating in real time and so it should be able to display the head outline within two frames.

\textbf{How test will be performed:} The person would be hidden behind the objects and then would repeatedly poke their head out. The system will then outline the head whenever it is in frame.The tester will measure the time by setting up a function to see how long the system takes to outline the person (from read input to having the coordinates for outline). If the time meets the minimal requirement to be considered a real time system.

\end{enumerate}

\subsubsection{Reliability}

\begin{enumerate}

\item{Same File Test (NFT21)\\}
  
\textbf{Control:} Manual

\textbf{Initial State:} The system is running in offline mode. A file with an object partially obstructing the person will be uploaded.

\textbf{Input:} A offline file with inputted multiple times

\textbf{Output:} The system displays a similar outline to the other inputs.

\textbf{Test Case Derivation:} Because the file is the same, uploading it multiple different times should result in the system outlining the human within the screen similarly. In the one file uploaded the human will be partially covered and so each time it is uploaded the coordinates for the outline should fit into a range that is only 5-10% off of previous upload.

\textbf{How test will be performed:} This will be done by having a file be uploaded to the system 10 times and then copy the results into an array to then measure how off it is to its original run. 
  
\end{enumerate}

\subsubsection{Accuracy}

\begin{enumerate}

\item{Live Data Test (NFT31)\\}

\textbf{Control:} Manual

\textbf{Initial State:} Kinect is properly set up whilst facing an environment with an object placed that is large enough to partially obstruct the view of a person. Software is running in realtime and is detecting the person in frame.

\textbf{Input:} Multiple Changed Positions

\textbf{Output:} The system displays the outline of the human within a certain visible range.

\textbf{Test Case Derivation:} The system is expected to produce an outline for the human that is relatively close to what can be seen on screen. The system should only detect what is visible and detect it in a given area.

\textbf{How test will be performed:} The person would hold a certain position and then the tester would manually check if the outlined area fits into a certain range by viewing the screen. The person would then change their position and the tester does the same process over again. This would be repeated 10 times to measure accuracy.
  
\end{enumerate}


\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[H]
  
  \centering
  \caption{Test and Requirements Traceability Matrix - See Section G.4 In \href{https://github.com/takhtart/PCD/blob/main/docs/SRS/SRS.pdf}{SRS Report}.}
  \begin{tabular}{|l|l|}
  \hline
  Tests   & Requirement \\
  \hline
  (FT11)  & {[}F411{]}  \\
  \hline
  (FT12)  & {[}F411{]}  \\
  \hline
  (FT13)  & {[}F411{]}  \\
  \hline
  (FT14)  & {[}F411{]}  \\
  \hline
  (FT15)  & {[}F411{]}  \\
  \hline
  (FT16)  & {[}F411{]}  \\
  \hline
  (FT17)  & {[}F411{]}  \\
  \hline
  (FT18)  & {[}F411{]}  \\
  \hline
  (FT21)  & {[}F412{]}  \\
  \hline
  (FT22)  & {[}F412{]}  \\
  \hline
  (FT23)  & {[}F412{]}  \\
  \hline
  (FT24)  & {[}F412{]}  \\
  \hline
  (FT25)  & {[}F412{]}  \\
  \hline
  (FT26)  & {[}F412{]}  \\
  \hline
  (FT27)  & {[}F412{]}  \\
  \hline
  (FT28)  & {[}F412{]}  \\
  \hline
  (FT31)  & {[}F413{]}  \\
  \hline
  (FT32)  & {[}F413{]}  \\
  \hline
  (FT41)  & {[}F414{]}  \\
  \hline
  (FT42)  & {[}F414{]}  \\
  \hline
  (FT43)  & {[}F414{]}  \\
  \hline
  (FT51)  & {[}F415{]}  \\
  \hline
  (FT52)  & {[}F415{]}  \\
  \hline
  (NFT11) & {[}NF431{]} \\
  \hline
  (NFT21) & {[}NF432{]} \\
  \hline
  (NFT31) & {[}NF433{]} \\ 
  \hline
  \end{tabular}
  \end{table}



\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
\textbf{Initial State:} 
					
\textbf{Input:} 
					
\textbf{Output:} \wss{The expected result for the given inputs}

\textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

\textbf{How test will be performed:} 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
\textbf{Initial State:} 
					
\textbf{Input:} 
					
\textbf{Output:} \wss{The expected result for the given inputs}

\textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

\textbf{How test will be performed:} 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
\textbf{Initial State:} 
					
Input/Condition: 
					
Output/Result: 
					
\textbf{How test will be performed:} 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
\textbf{Initial State:} 
					
\textbf{Input:} 
					
\textbf{Output:} 
					
\textbf{How test will be performed:} 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.


\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\end{document}